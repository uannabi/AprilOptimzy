{"id":161484,"url":"http://en.wikipedia.org/wiki/Graphics_processing_unit","text":"A graphics processing unit (GPU) is a processor that renders (or creates) images, animations, and graphics and displays them for the computer\u2019s screen. A strong GPU is able to process complex animations and graphics smoothly and efficiently.\n2D and 3D.\nThere are generally two types of images which GPU has to calculate, which is also called rendering. Most applications like the operating system\u2019s desktop (except e.g. Windows Vista\u2019s Aero Desktop) or office applications, the GPU has to render two dimensional (2D) pictures. Modern GPUs are so powerful that there is no difference in 2D performance between low-cost and expensive GPUs.\nSome applications use 3D pictures to simulate three-dimensionality. Examples for such applications are computer and video games or applications for design and technical construction (CAD). The 3D acceleration performance of GPUs differs greatly. Generally, more expensive GPUs can render faster than low-cost ones.\nManufacturers.\nNvidia, AMD, and Intel are the most common designers of graphics cards. However, many manufacturers work with these three companies to manufacture the actual hardware. These manufacturers include, but are not limited to, Asus, MSI, and EVGA. AMD and Intel sometimes include integrated GPUs embedded in their CPUs but these are not as powerful as a dedicated external GPU that is seperate from the CPU.\nOrigin.\nThe first graphics cards were used in the 1970\u2019s within arcade machines as a cheaper alternative to random access memory (RAM). However, graphics cards were not viable for consumer products until the release of the Large Scale Integration (LSI) circuit chip in the 1980\u2019s. During the early-to-mid 1990\u2019s, graphics cards that were capable of 3D support became more common in arcade, computer, and console games. The first consumer-level GPU meant for personal computers was the Nvidia GeForce 256, which was released in 1999.\nModern Uses.\nGPUs are commonly used in computers and video game systems to render graphics for video games that may not be playable without the proper GPU. However, other devices such as virtual reality headsets and driverless cars also use GPUs. GPU computing, which every tasks that a GPU can do, includes calculations, artificial intelligence programming, photo editing, and other applications.\nGPU Computing.\nThe term GPU computing combines all tasks a GPU can calculate that go beyond simple calculation and output of pictures. It is also known under the term General Purpose Computation on Graphics Processing Unit. Tasks are calculation of physics, Artificial Intelligence or even acceleration of video and picture editing. One of the first applications to support GPU computing is Adobe\u2019s Photoshop CS4.\nnVidia\u2019s approach: CUDA.\nnVidia calls their attempt at GPU Computing CUDA. This is nVidia's interface for using their GPUs for general computing tasks. CUDA is based on the C programming language.\nATi\u2019s approach: Stream.\nATi is calling their attempt Stream. There were some earlier attempts under a different name prior to Stream\u2019s release with ATi\u2019s graphics driver \u201CCatalyst 8.12\u201D in December 2008.\nS3 Graphic\u2019s approach.\nS3 Graphics showed a first application for video editing which is accelerated by their latest GPUs. Yet, there is no information whether S3 Graphics will continue to increase their efforts in GPU computing.\nThe latest milestone in GPU computing was AMD\u2019s announcement of a Super-Computer based on GPUs at the CES in January 2009. It will be built of more than a thousand GPUs and will have a computing power of one petaflop.\nExternal GPU.\nAn external GPU is a graphics processor located outside of the housing of the computer. External graphics processors are used with laptop computers, which lack a powerful graphics processor. On-board graphics chips are not powerful enough for graphically intensive tasks. Therefore, attaching a GPU to some notebook is desirable.\nGPU Acclerated Video Decoding.\nMost GPUs made since 1995 support YUV color space and hardware overlays, and many GPUs made since 2000 also support MPEG primitives. This process of hardware accelerated video decoding. Most recent graphics cards even decode high-definition video on the card, offloading the central processing unit.\nGPU Interfaces.\nEarly GPUs used Peripheral Component Interconnect (PCI) interface to communicate with the motherboard of a computer, a type of slot which is still used today for simple motherboard add-ons such as extra audio-processing cards or USB ports. As graphics cards became more powerful and graphics applications more demanding, a faster kind of interface called Advanced Graphics Port (AGP) was created and then PCIe x16 (PCI Express 16-lane) after it, the current overall standard. In 2008 PCIe 2.0 was introduced, doubling the speed of PCIe 1.0 and still working with older PCI Express interfaces.\nPCI graphics cards will only work in PCI slots; AGP will only work in AGP slots. PCIe graphics cards will not function in PCI slots and PCI graphics cards will not function in PCIe slots. However, PCI Express 2.0 graphics cards will work in PCI Express x16 1.0 motherboard slots and vice-versa.\nMulti-GPU systems.\nMulti-GPU Systems are computers which use more than one GPU. Generally this is used in high-end home computers to accelerate computer games, but there is also the ability to have one GPU rendering normal game scenes and one GPU calculating physics. This is currently supported by nVidia\u2019s GPUs only and called PhysX. ATi and Intel have developed their own physics engine named Havok.\nHistory of Multi-GPU systems.\nThe first attempts in Multi-GPU Systems were done by 3Dfx, which was bought by nVidia later. Their Voodoo video cards, which were 3D only accelerators, could be hooked up to a second Voodoo video card to gain more performance. This was called SLi. Theoretically the performance would double, practically it increased far less, depending on the video game, which is still a problem in today\u2019s Multi-GPU Systems. Later, 3Dfx built more than one GPU on one video card. It\u2019s latest video card, the Voodoo 6, was based on four GPUs, but it was never released. However there exist several engineering samples of the Voodoo 6, mostly owned by collectors.\nnVidia\u2019s SLi.\nAfter nVidia bought 3Dfx, Multi-GPU Systems were dead for several years until nVidia came up with this idea again in 2004. As nVidia had bought all technologies and market names from 3Dfx, too, they simply used the name SLi again. Today\u2019s SLi works with up to three video cards, which is called 3-way-SLi. There were also attempts with four cards, but this failed due to limitations in the DirectX9 API. Today\u2019s DirectX10 API however would support this.\nDespite the video cards, a SLi compatible motherboard is needed to build a SLi-System. Such motherboards have several PEG Slots (the successor to AGP, based on PCI-Express Technology) for more than one video card.\nnVidia offers various Multi-GPU video cards, where two GPUs are built on one video card. These are often called Dual-GPU video cards and do not need a SLi compatible motherboard.\nATi\u2019s CrossFireX.\nATi\u2019s Multi-GPU Technology is called Cross Fire. It works quite the same way, though there are fewer limitations for video card combinations. CrossFire was renamed as CrossFire X in 2007 to represent the possibility to combine more than two cards. Today, it is possible to have a CrossFire X system based on four video cards or two Dual-GPU cards.\nJust like with SLi, there is the necessity for a CrossFire compatible motherboard in order to build a CrossFire System. One Dual-GPU video card from ATi does not necessarily need such a Crossfire compatible motherboard, however for two of them, it is needed.","categories":[],"infobox_types":[],"annotations":[{"uri":"Operating_system","surface_form":"operating system","offset":380},{"uri":"Windows_Vista","surface_form":"Windows Vista","offset":420},{"uri":"Desktop","surface_form":"Desktop","offset":441},{"uri":"2D","surface_form":"2D","offset":513},{"uri":"3D","surface_form":"3D","offset":660},{"uri":"Video_game","surface_form":"video game","offset":754},{"uri":"GPGPU","surface_form":"General Purpose Computation on Graphics Processing Unit","offset":2468},{"uri":"Physics","surface_form":"physics","offset":2550},{"uri":"Artificial_intelligence","surface_form":"Artificial Intelligence","offset":2559},{"uri":"Adobe","surface_form":"Adobe","offset":2692},{"uri":"Photoshop","surface_form":"Photoshop","offset":2700},{"uri":"CUDA","surface_form":"CUDA","offset":2784},{"uri":"C_(programming_language)","surface_form":"C programming language","offset":2888},{"uri":"Stream","surface_form":"Stream","offset":2965},{"uri":"Floating_Point_Operations_Per_Second","surface_form":"petaflop","offset":3554},{"uri":"PhysX","surface_form":"PhysX","offset":5472},{"uri":"Havok","surface_form":"Havok","offset":5539},{"uri":"SLi","surface_form":"SLi","offset":5819},{"uri":"SLi","surface_form":"SLi","offset":6477},{"uri":"PCI-Express_for_Graphics","surface_form":"PEG","offset":6810}]}