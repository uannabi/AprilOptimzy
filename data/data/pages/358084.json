{"id":358084,"url":"http://en.wikipedia.org/wiki/Backpropagation","text":"e=\"preserve\">Backpropagation is a method of training neural networks to perform tasks more accurately. The algorithm was first used for this purpose in 1974 in papers published by Werbos, Rumelhart, Hinton, and Williams. The term backpropagation is short for \"backward propagation of errors\".\nIt works especially well for feed forward neural networks (networks without any loops) and problems that require supervised learning.\nHow it works.\nThe idea is to test how wrong the neural network is and then correct it. This is repeated many times.\nThis is repeated until the neural network is good enough at its job -i.e., its error as measured by the loss function is low.","categories":[],"infobox_types":[],"annotations":[{"uri":"Neural_network","surface_form":"neural network","offset":53},{"uri":"Paul_Werbos","surface_form":"Werbos","offset":180},{"uri":"David_E._Rumelhart","surface_form":"Rumelhart","offset":188},{"uri":"Geoffrey_E._Hinton","surface_form":"Hinton","offset":199},{"uri":"Ronald_J._Williams","surface_form":"Williams","offset":211},{"uri":"Supervised_learning","surface_form":"supervised learning","offset":406}]}