{"id":610032,"url":"http://en.wikipedia.org/wiki/Reinforcement_learning","text":"e=\"preserve\">: \"The biology behind Reinforcement learning can be found at Operant conditioning, and Reward\"\nReinforcement learning (RL) is teaching a \"software agent\" how to behave in an environment by telling it how good it's doing. It is an area of machine learning inspired by behaviorist psychology.\nReinforcement learning is different from supervised learning because the correct inputs and outputs are never shown. Also, reinforcement learning usually learns as it goes (online learning) unlike supervised learning. This means an agent has to choose between exploring and sticking with what it knows best.\nIntroduction.\nA reinforcement learning system is made of a \"policy\" (formula_1), a \"reward function\" (formula_1), a \"value function\" (formula_1), and an optional \"model\" of the environment.\nA \"policy\" tells the agent what to do in a certain situation. It can be a simple table of rules, or a complicated search for the correct action. Policies can even be stochastic, which means instead of rules the policy assigns \"probabilities\" to each action. A policy by itself can make an agent do things, but it can't learn on its own.\nA \"reward function\" defines the goal for an agent. It takes in a state (or a state and the action taken at that state) and gives back a number called the \"reward\", which tells the agent how good it is to be in that state. The agent's job is to get the biggest amount of reward it possibly can in the long run. If an action yields a low reward, the agent will probably take a better action in the future. Biology uses reward signals like pleasure or pain to make sure organisms stay alive to reproduce. Reward signals can also be stochastic, like a slot machine at a casino, where sometimes they pay and sometimes they don't.\nA \"value function\" tells an agent how much reward it will get following a policy formula_1 starting from state formula_1. It represents how \"desirable\" it is to be in a certain state. Since the value function isn't given to the agent directly, it needs to come up with a good guess or estimate based on the reward it's gotten so far. Value function estimation is the most important part of most reinforcement learning algorithms.\nA \"model\" is the agent's mental copy of the environment. It's used to \"plan\" future actions.\nKnowing this, we can talk about the main loop for a reinforcement learning episode. The agent interacts with the environment in \"discrete time steps\". Think of it like the \"tick-tock\" of a clock. With discrete time, things only happen during the \"ticks\" and the \"tocks\", and not in between. At each time formula_1, the agent observes the environment's state formula_1 and picks an action formula_1 based on a policy formula_1. The next time step, the agent receives a reward signal formula_1 and a new observation formula_1. The value function formula_1 is updated using the reward. This continues until a terminal state formula_1 is reached.","categories":[],"infobox_types":[],"annotations":[{"uri":"Biology","surface_form":"biology","offset":20},{"uri":"Operant_conditioning","surface_form":"Operant conditioning","offset":74},{"uri":"Reward","surface_form":"Reward","offset":100},{"uri":"Software_agent","surface_form":"software agent","offset":151},{"uri":"Machine_learning","surface_form":"machine learning","offset":251},{"uri":"Behaviorism","surface_form":"behaviorist psychology","offset":280},{"uri":"Supervised_learning","surface_form":"supervised learning","offset":345},{"uri":"Slot_machine","surface_form":"slot machine","offset":1687}]}